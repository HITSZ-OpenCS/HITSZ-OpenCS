## 哈尔滨工业大学（深圳）2022级模式识别期末复习指南

### 声明

本资料由本人独自整理，水平有限，可能会出现错误。

### 复习须知

1.复习资料的选择：**只看PPT可以过但拿不到高分**。

确实如学长所说，模式识别的考题中有不少**送分题**。但是，也**有一些选项PPT中根本没有**。

>本人模式识别课的老师是zzg，不清楚其他老师的 PPT中是否有这些选项。
>
>如果其他老师的PPT中有这些选项的话，22级的学弟学妹们在复习时可以互换一下PPT，避免遗漏考点。
>
>如果没有的话，那想要拿高分就不能只看PPT了。

2.考查方式：主要考**对知识点的理解**，**几乎**不用背公式，不用背算法，不用计算。

>比如对于贝叶斯公式、贝叶斯决策、最大似然估计这些，**不要只会计算，只会计算的话那做不对题**。
>
>**虽然老师说了不用背公式，但填空第1题还是考了个贝叶斯公式的默写**，因此我在前面写了“几乎不用背公式”而不是“不用背公式”。

3.考查范围：老师期末复习PPT中标important的重点就是全部的大考点（但一些属于这个大考点的知识点PPT里没有），PPT中没标important的完全不用看。

>本人模式识别课的老师是zzg，其他老师的PPT的标记方式可能会有差异。


### 题型分布

>注意：若2022级期末的题型分布发生了变化，请以最新题型分布为准。

题型分为**判断题、填空题和选择题**，占比分别约为**10%、10%和80%**，其中选择题全部为**单项选择题**，选项个数为**2、3或4**不等。

2022秋与2023秋的具体分值如下：
|     | 2022秋  |2023秋  |
|  ----  | ----  |----  |
| 判断题  | 14分 |10分 |
| 填空题  | 8分 |13分 |
| 选择题  | 78分 |77分（2.2分×35） |

### 复习题（2023秋期末试题回忆版，按知识点分类）

>注意：若2022级期末的考试范围发生了变化，请以最新考试范围为准。

>注：回忆版试题中，无人驾驶和过拟合那两道题的题干并没有被回忆出来，只是为了还原原卷的感觉而在网上找素材补充的，不保证材料与原题一致，但不影响做题。千人千面和乳腺癌那两道题是根据大概意思回忆的。

#### 考点1 模式识别基本概念
1.模式识别算法设计的目标是

A. 目标是研究普遍性的算法，适用于大部分情况，这样后人就可以直接拿来用

B. 目标是研究特定的任务或特定一类任务的算法


><font face="楷体">【答案】或许是A
>
>【备注】这个知识点PPT里没有。个人的理解是，我们考试范围内的KNN、PCA、FDA等，以及考试范围外的BP、SVM等，都是具有普遍性的算法，而不是只能用于产品质量检验、设备故障检测、钱币的自动识伪、自动进行疾病的诊断、生理特征鉴别、文字识别等等这些特定场景中的一类。</font>

2.下列关于算法和模型的说法中，错误的是

· 对于复杂的模式识别问题，选择复杂度高的算法，不用考虑其他的因素

><font face="楷体">【答案】或许是选这个选项
>
>【备注】这个知识点PPT里没有，不确定答案是什么。但“不用考虑其他的因素”看起来太绝对了。</font>

3.无人驾驶技术是指在无需人类干预的情况下，自主实现车辆控制与操作，从而实现汽车驾驶自动化的技术。目前，无人驾驶技术已经广泛应用于物流配送、公共交通、城市运输等领域，成为智能交通领域的研究热点之一，正日益成为未来交通发展的重要方向。人们之所以对此有担忧是因为

C. 尽管无人驾驶的算法在不断提升，但在实际应用中还是会出现一些问题

><font face="楷体">【答案】C</font>

4.人们青睐深度学习不是因为

A. 深度学习是最优的方法

（其他选项一眼对）

><font face="楷体">【答案】A</font>

5.下列说法中正确的是

B. 未来在深度学习算法上会有一定进展

C. 人工智能会超越人类，完全智能化

><font face="楷体">【答案】B</font>

6.下列说法中错误的是

· 人工智能会有100%准确率

><font face="楷体">【答案】·</font>

#### 考点2 距离分类器
1.判断：确定的xxx没必要设计分类器

><font face="楷体">【备注】题干记不全了，没办法给出答案，不过这个知识点好像PPT中也没有。</font>

2．下列关于?的说法中正确的是

C. 为了充分利用样本，可以把相似度比较高的其他类别的样本拿过来用	

><font face="楷体">【答案】C
>
>【解析】这属于困难负样本
>
>【备注】这个知识点PPT里没有，但凭直觉可以选出来
>
>【参考链接】https://zhuanlan.zhihu.com/p/345223357</font>

3.下列关于K近邻的说法中，正确的是

A. 难点是?

C. 由距离最近的K个样本的类别决定

><font face="楷体">【答案】C</font>

4.下列说法中正确的是

·K近邻法中，样本多时，K要选得大一些

><font face="楷体">【答案】不选这个选项
>
>【解析】这个知识点PPT里没有。KNN中K的选择是一个复杂的问题，更多内容可查阅下方的参考资料
>
>【参考资料】
>https://www.researchgate.net/post/How_can_we_find_the_optimum_K_in_K-Nearest_Neighbor
>
>https://stats.stackexchange.com/questions/534999/why-is-k-sqrtn-a-good-solution-of-the-number-of-neighbors-to-consider</font>

5.过拟合尤其容易在训练迭代次数相对有限训练样本过多的时候。此时，模型会拟合训练数据中特征的随机噪声，而这些与目标函数之间并无因果关系。在这种过拟合的过程中，模型在训练样本上的效果会持续提升，但在训练中不可见的数据（通常是验证集）上的效果会变得更差。下列情况中，类似于“过拟合”的是

A. 题海战术 

B. 做大量的题然后对答案根据答案不断学习

C. 做有限的题

><font face="楷体">【答案】B</font>

6.下列说法中正确的是

·分类的类别数目多会导致准确率下降

><font face="楷体">【答案】不选这个选项
>
>【解析】这个知识点PPT里没有，分类的类别数目多不一定会导致准确率下降
>
>【参考资料】https://arxiv.org/pdf/1506.01567.pdf</font>

7.判断：无监督学习的类别不确定，因此没办法评估准确率

><font face="楷体">【答案】√
>
>【备注】这个知识点PPT里没有，凭直觉选可能会选错。
>
>【参考资料】Since there is no pre-evidence or records for patterns, we cannot directly compute the accuracy by comparing actual and predicted outputs but there exist many evaluation metrics to measure the performance of unsupervised learning algorithms after the training process.
>
>https://www.quora.com/How-do-you-evaluate-the-performance-of-an-unsupervised-learning-algorithm</font>

8.千人千面，世界上没有完全相同的两片叶子，一千个人之中也找不到两个长相完全一样的人。每个人的外貌都不尽相同，当我们要辨别的人少时，我们还能够比较轻松地区分每个人，但人多了之后，我们分辨出每个人更困难了，这体现出

C. 需要分类的类别数目多会导致模式识别任务变得更复杂

><font face="楷体">【答案】C
>
>【解析】答案就是把题干翻译了一遍，无需联系其他知识点。</font>

#### 考点3 最小均方误差

1.判断：最小均方误差只适用于线性可分的情况

><font face="楷体">【答案】×
>
>【解析】算法对于线性不可分的训练样本也能够收敛于一个均方误差最小解（PPT第4章第26页）</font>

2.下列关于最小均方误差的说法中，正确的是

A. 不适用多值情况

C. 由于？少，效果不会好

><font face="楷体">【答案】A
>
>【解析】这个知识点PPT上没有，MSE适用于回归问题，BCE适用于分类问题
>
>【参考资料】
https://www.youtube.com/watch?v=bNwI3IUOKyg&ab_channel=WhyML</font>

#### 考点4 PCA和LDA

1.下列关于PCA的说法中，错误的是

B. 维数不能为特征原始维度d

C. 主成分是原始变量的线性组合

><font face="楷体">【答案】B
>
>【解析】最大维度为d（PPT第5章第79页）</font>

2.下列关于LDA的说法中，正确的是

·xxxxc-1

><font face="楷体">【备注】选项忘了</font>

3.下列关于PCA的说法中，正确的是

D.因为要对测试集进行分类，因此只需要看测试集中的类别即可

><font face="楷体">【备注】应该不选这个？</font>

4.LDA的投影具有物理意义。LDA的特征向量个数是

A. c 

B. c-1 

C. 1

><font face="楷体">【备注】题干中没说至少，但除了B之外也没别的可选</font>

5.判断：主成分分析法是无监督学习，因此之后不能再用K近邻

><font face="楷体">【答案】×
>
>【解析】这个知识点PPT上没有，可以利用PCA进行降维，然后再进行监督学习
>
>【参考资料】
>
>https://alexmiller.phd/posts/how-to-semi-supervised-learning-pca-principle-component-analysis/
>
>Now that we have a k-dimensional representation of our training data X, you can train your favorite classifier (SVM, kNN, logistic regression, etc.) on the transformed features ^Z
>
>https://stackoverflow.com/questions/20256736/how-do-i-use-principal-component-analysis-in-supervised-machine-learning-classif
>
>https://datascience.stackexchange.com/questions/985/can-i-use-unsupervised-learning-followed-by-supervised-learning
>
>Using unsupervised learning to reduce the dimensionality and then using supervised learning to obtain an accurate predictive model is commonly used. See for example Bhat and Zaelit, 2012 where they first use PCA to reduce the dimension of a problem from 87 to 35. Then, they use L1 regression to obtain the best predictive model.
>
>https://askdatascience.com/832/can-pca-be-used-for-supervised-learning
>
>PCA can be used indirectly in supervised learning tasks such as classification and regression. When you have huge number of features, one way to reduce the number of features and probably avoid overfitting is using a feature reduction method such as PCA. Therefore, PCA can be used in preprocessing step to reduce the number of features.</font>

6.LDA的特征向量的方向指示__________（5分）

><font face="楷体">【备注】只知道相关知识点是第一投影方向是类别差异性最大的方向，但题干好像没单独问第一投影方向</font>

注：记得某道题的某个选项里有“先求协方差，然后计算特征向量”这种话，但记不清是哪道题里的了

7.下列说法中正确的是

B. xxx可以映射到(-1,1)

C. 如果映射到(0,1)，xxx

><font face="楷体">【备注】记不清这道题具体怎么考的了，可能相关的链接https://blog.csdn.net/Goodbye_/article/details/104428852</font>

#### 考点5 贝叶斯决策

1.判断：P(ω<sub>1</sub>|x)>0.5 ，根据贝叶斯最小错误决策，分到一类

><font face="楷体">【答案】√</font>

2.R(α<sub>i</sub>|x)是_______，之后的做法是________（第一空2分，第二空3分）

><font face="楷体">【答案】决策α<sub>i</sub>的条件风险；找出有最小条件风险的决策
>
>【备注】这个知识点在老师lecture16中的拓展链接里有</font>

3.2013年，安吉丽娜朱莉在自己写的书中称自己通过基因检测确定带遗传缺陷基因，为了降低患癌风险，朱莉便做了乳腺切除手术。对此的看法正确的是

A. 每个人的个体情况不一样，即使带有遗传缺陷也不代表一定发病，完全没必要切除乳腺

C. 这是规避风险的做法

><font face="楷体">【答案】C</font>

4.下列属于基于最小平均风险的贝叶斯决策的是 

A. 两权相害取其轻,两利相权取其重

B. 勿以善小而不为，勿以恶小而为之

C. 屋漏偏逢连夜雨

><font face="楷体">【答案】A</font>

5.下列说法中错误的是

A. 基于最小错误的贝叶斯决策的准确率比基于最小平均风险的贝叶斯决策高

><font face="楷体">【答案】A</font>

6.在贝叶斯最小风险分类中，说法错误的是

B.动作(action)是不必要的

C.动作可以是拒绝(reject)

><font face="楷体">【答案】B
>
>【备注】zzg老师的PPT中没有“动作”相关的概念，但这道题可以凭直觉选</font>

7.在贝叶斯最小风险分类中，说法正确的是

A. 最终选择的动作，损失函数是0 

B. 最终选择的动作，损失函数不是0 

C. 要让风险最小 

D. 要让风险和条件风险都最小

><font face="楷体">【答案】B？</font>


8.蚂蚁搬家要下雨，这是

A. 根据似然推后验概率的贝叶斯估计

B. 基于最小错误的贝叶斯决策

C. 基于最小平均风险的贝叶斯决策

><font face="楷体">【答案】A</font>

9.基于我们对现实世界的观察，我们了解到穿裙子的人大部分是女生。因此，当我们只看到一个人穿裙子而没有看到其他部位时，我们可以推断出这个人很可能是女生，这用到了

A.先验概率越大，后验概率往往也越大

C 后验概率由似然和先验概率共同决定

><font face="楷体">【答案】C
>
>【解析】这个人是男生和女生的先验概率都是50%左右，不能选A</font>

10.已知p(x)、P(x|ω<sub>i</sub>)、P(ω<sub>i</sub>)，则后验概率P(ω<sub>i</sub>|x)为_____________。（3分）

><font face="楷体">【答案】
>$$
>P\left( \omega _i|x \right) =\frac{p\left( x|\omega _i \right) P\left( \omega _i \right)}{p\left( x \right)}
>$$
>
>【备注】说了不用记公式，但还是考了贝叶斯公式</font>

11.东北毒蛇少，所以被蛇咬后一般不用注射血清，这体现了

A.后验概率与先验概率往往正相关

><font face="楷体">【答案】A</font>

12.下列说法中正确的是

·如果已经知道后验概率，那就没必要求先验概率和似然了

><font face="楷体">【备注】不知道这个选项有没有什么坑，没坑的话那它就是正确选项</font>

#### 考点6 概率密度函数估计

1.判断：参数估计是无监督学习（）非参数估计是有监督学习（）后者适用的情况更普遍（）

><font face="楷体">【答案】× √ √
>
>【解析】参数估计分为无监督和有监督两种，而非参数估计都是有监督。（PPT第7章第80页）一般情况下,非参数估计方法较为通用,适用范围更广（根据PPT中的“对分布函数无任何假设”凭直觉也可以猜出来）。></font>

2.下列关于参数估计和非参数估计的说法中，正确的是

A. 参数估计计算量小于非参数估计	
			
C. 参数估计和非参数估计一般分别用于符合正态分布的和非正态分布的情况

><font face="楷体">【答案】A
>
>【备注】C选项没有这种说法，但有点唬人。</font>

3.下列说法中正确的是

参数估计一般用的是正态分布

><font face="楷体">【备注】没有这种说法，但这选项有点唬人。</font>

4.下列关于最大似然估计的说法中正确的是

A.最大似然估计随着样本增多，准确率会提升

><font face="楷体">【答案】A
>
>【解析】PPT第7章第108页</font>

5.最大似然估计的主要思想是

A. 如果在一次观察中一个事件出现了，则我们可以认为这一事件出现的可能性很大，需要计算让联合概率最大

B. 寻求使后验概率最大的参数值

><font face="楷体">【答案】A
>
>【解析】PPT第7章第108页
>
>【备注】B选项为最大后验估计，但不知道这个不影响做对题，复习时注意掌握对最大似然估计的深刻理解，而不是只知道对乘积取对数然后求导，这样是做不对题的。</font>

6.下列说法中错误的是

B. 非参数估计中选择的区域的大小与样本数无关

C. n趋于无穷时xxx

><font face="楷体">【答案】B
>
>【解析】PPT第7章第85页</font>

7.要对某年龄的男生身高作参数估计，错误的是

A. 参数估计的参数指的就是每一点的概率密度

B. 这是将离散的转换为连续的 

C. 不适合用参数估计

><font face="楷体">【答案】？
>
>【解析】A选项估计的应该是均值和标准差，C选项，同年龄男生的身高是符合正态分布的，适合用参数估计

>【备注】这道题记得比较清楚，题干问的就是错误的选项，问的不是正确的选项</font>

8.下列关于Parzen窗法和kn-近邻法的说法中，错误的是       

A. Parzen窗法的效果比kn-近邻法差 

><font face="楷体">【备注】Parzen窗法的优点：普遍适应，对规则或不规则分布，单锋或多峰分布都可适用（PPT第7章第99页）</font>

9.下列关于kn-近邻法的说法中，正确的是                  
				
D. k的选择要根据具体情况而定

><font face="楷体">【答案】D</font>

10.判断：概率密度函数的积分一般为1

><font face="楷体">【答案】√</font>










